{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ali\n",
      "[nltk_data]     Doğan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Ali\n",
      "[nltk_data]     Doğan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ali Doğan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sent_tokenize', 'function', 'uses', 'an', 'instance', 'of', 'PunktSentenceTokenizer', 'from', 'the', 'nltk.tokenize.punkt', 'module', ',', 'which', 'is', 'already', 'been', 'trained', 'and', 'thus', 'very', 'well', 'knows', 'to', 'mark', 'the', 'end', 'and', 'beginning', 'of', 'sentence', 'at', 'what', 'characters', 'and', 'punctuation', '.']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenizer\n",
    "text =\"The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk.tokenize.punkt module, which is already been trained and thus very well knows to mark the end and beginning of sentence at what characters and punctuation.\"\n",
    "words=word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing (NLP) is a subfield of computer science, artificial intelligence, information engineering, and human-computer interaction.',\n",
       " 'This field focuses on how to program computers to process and analyze large amounts of natural language data.',\n",
       " 'It is difficult to perform as the process of reading and understanding languages is far more complex than it seems at first glance.',\n",
       " 'Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.',\n",
       " 'One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentences Tokenizer\n",
    "text=\"Natural Language Processing (NLP) is a subfield of computer science, artificial intelligence, information engineering, and human-computer interaction. This field focuses on how to program computers to process and analyze large amounts of natural language data. It is difficult to perform as the process of reading and understanding languages is far more complex than it seems at first glance. Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\"\n",
    "\n",
    "\n",
    "sentences=sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural_Language_Processing', '(NLP)', 'is', 'a', 'subfield_of', 'computer', 'science,', 'artificial', 'intelligence,', 'information', 'engineering,', 'and', 'human-computer', 'interaction.']\n"
     ]
    }
   ],
   "source": [
    "#Multiword Tokenizer\n",
    "tk = MWETokenizer([ ('Natural','Language','Processing')]) \n",
    "tk.add_mwe(('subfield','of'))\n",
    "text=\"Natural Language Processing (NLP) is a subfield of computer science, artificial intelligence, information engineering, and human-computer interaction. \"\n",
    "mwtoken = tk.tokenize(text.split()) \n",
    "   \n",
    "print(mwtoken) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
      "The striped bat are hanging on their foot for best\n"
     ]
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "# Define the sentence to be lemmatized\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)\n",
    "\n",
    "# Lemmatize list of words and join\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(lemmatized_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hang\n",
      "hanging\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"hanging\", 'v'))  \n",
    "\n",
    "\n",
    "print(lemmatizer.lemmatize(\"hanging\", 'n')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('feet', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(['feet']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('striped', 'JJ'), ('bats', 'NNS'), ('are', 'VBP'), ('hanging', 'VBG'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('for', 'IN'), ('best', 'JJS')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(nltk.word_tokenize(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#main tags are here.\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "#  Lemmatize Single Word with the appropriate POS tag\n",
    "word = 'getting'\n",
    "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "#  Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
